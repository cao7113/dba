#!/usr/bin/env ruby
require 'yaml'
require 'etc'
require_relative '../boot'

class PgaCLI < Thor
  class_option :dryrun, type: :boolean, default: false, banner: 'dry run', aliases: [:d]
  class_option :verbose_cmd, type: :boolean, default: true, banner: 'run cmd verbosely'
  class_option :filter_big_data, type: :boolean, default: true, banner: 'filter big data table'
  class_option :datafile, banner: 'dump data rule file'
  class_option :use_datafile, type: :boolean, default: true
  # FIXME 为解决命令互调时判断是否是初始触发命令
  class_option :_init_task, type: :boolean, default: true, banner: 'init task'
  class_option :dump_opts, banner: 'dump extra options'

  desc 'dump_ddl src_url [dump_dir]', 'dump ddl to a sql file'
  def dump_ddl(src_url, dump_dir = nil)
    cmds = []
    src_conn, dump_dir, cmd = handle_dump_args(src_url, dump_dir)
    cmds << cmd

    sqlfile = File.join(dump_dir, 'ddl.sql')
    cmds << "#{dump_cmd} --schema-only #{dump_opts} --no-owner --no-privileges #{verbose_cmd_opts} --file=#{sqlfile} #{src_conn.url}"

    handle_cmds(cmds)

    {
      src_conn: src_conn,
      sqlfile: sqlfile,
      cmds: cmds
    }
  end

  desc 'restore_ddl dest_url sqlfile', 'load sql on dest_url'
  def restore_ddl(dest_url, sqlfile)
    cmds = []
    check_url!(dest_url)
    ensure_no_dest_db(dest_url)
    unless dryrun
      dest_conn = Dba::ConnectionBuilder.new(url: dest_url)
      create_db_before_restore(dest_conn) 
    end
    cmds << "#{sql_cmd} --file=#{sqlfile} --single-transaction --dbname #{dest_url}"
    handle_cmds(cmds)
  end

  desc 'copy_ddl src_url dest_url', 'copy ddl between dbs'
  def copy_ddl(src_url, dest_url)
    cmds = []
    dest_url = allow_local_db_url(dest_url)
    check_url!(dest_url)
    ensure_no_dest_db(dest_url)
    src_conn, dump_dir, cmd = handle_dump_args(src_url, dump_dir)

    ddl_info = invoke(:dump_ddl, [src_conn, dump_dir], options.merge(_init_task: false))
    cmds = invoke(:restore_ddl, [dest_url, ddl_info[:sqlfile]], options.merge(_init_task: false))

    cmds = ddl_info[:cmds] + cmds 
    handle_cmds(cmds)
  end

  desc 'dump_data src_url [dump_dir]', 'dump data to a dir'
  def dump_data(src_url, dump_dir = nil)
    cmds = []
    src_conn, dump_dir, cmd = handle_dump_args(src_url, dump_dir)
    cmds << cmd

    data_dir = File.join(dump_dir, 'data')

    dfile = dump_datafile(src_conn)
    if dfile
      ex_opts = exclude_opts(dfile)
    end

    if !dfile && options[:filter_big_data]
      hint_size = 500 * 1024 ** 2 # 500M
      if src_conn.dbsize >= hint_size 
        abort <<-Hint
          No dump datafile for #{src_conn.dbsize/10**6}M #{src_conn.dbname}!"
          refer sql/smart_opts.rb to get idea!
        Hint
      end
    end

    cmds << "#{dump_cmd} --no-owner --data-only #{jobs_opts} --format=d --file=#{data_dir} #{verbose_cmd_opts} #{ex_opts} #{dump_opts} #{src_conn.url};"

    handle_cmds(cmds)
    {
      cmds: cmds,
      data_dir: data_dir
    }
  end

  desc 'restore_data dest_url data_dir', 'restore data only from data dir'
  def restore_data(dest_url, data_dir)
    cmds = []
    check_url!(dest_url)
    dest_conn = Dba::ConnectionBuilder.new(url: dest_url)
    unless dryrun
      if options[:_init_task]
        abort "#{dest_url} not exists!" unless dest_conn.db_not_exist?
        abort "Not found #{data_dir}!" unless File.directory?(data_dir)
      end
    end

    cmds << "#{restore_cmd} --no-owner --data-only --format=d #{jobs_opts} #{verbose_cmd_opts} --dbname=#{dest_url} #{data_dir};"
    handle_cmds(cmds)
  end

  # copy table data: --dump-opts='--table fundings' --dryrun
  desc 'copy_data SRC_URL DEST_URL', 'copy data between dbs'
  def copy_data(src_url, dest_url)
    cmds = []
    dest_url = allow_local_db_url(dest_url)
    data_info = invoke(:dump_data, [src_url], options.merge(_init_task: false, use_datafile: false))
    cmds += data_info[:cmds]
    cmds += invoke(:restore_data, [dest_url, data_info[:data_dir]], options.merge(_init_task: false))
    handle_cmds(cmds)
  end

  desc 'dump src_url [dump_dir]', 'dump db to a dir'
  def dump(src_url, dump_dir = nil)
    src_conn, dump_dir, cmd = handle_dump_args(src_url, dump_dir)
    ddl_info = invoke(:dump_ddl, [src_conn, dump_dir], options.merge(_init_task: false))
    data_info = invoke(:dump_data, [src_conn, dump_dir], options.merge(_init_task: false))
    cmds = ddl_info[:cmds] + data_info[:cmds]
    handle_cmds(cmds)
  end

  desc 'restore dest_url dump_dir', 'restore from dump dir'
  def restore(dest_url, dump_dir)
    cmds = []
    dest_url = allow_local_db_url(dest_url)
    check_url!(dest_url)
    ensure_no_dest_db(dest_url)

    sqlfile = File.join(dump_dir, 'ddl.sql')
    data_dir = File.join(dump_dir, 'data')
    cmds += invoke(:restore_ddl, [dest_url, sqlfile], options.merge(_init_task: false))
    cmds += invoke(:restore_data, [dest_url, data_dir], options.merge(_init_task: false))

    handle_cmds(cmds)
  end

  desc 'copy src_url dest_url', 'copy db between dbs'
  def copy(src_url, dest_url)
    cmds = []
    dest_url = allow_local_db_url(dest_url)
    check_url!(dest_url)
    ensure_no_dest_db(dest_url)
    src_conn, dump_dir, cmd = handle_dump_args(src_url, dump_dir)

    ddl_info = invoke(:dump_ddl, [src_conn, dump_dir], options.merge(_init_task: false))
    cmds += ddl_info[:cmds]

    data_info = invoke(:dump_data, [src_conn, dump_dir], options.merge(_init_task: false))
    cmds += data_info[:cmds]

    cmds += invoke(:restore_ddl, [dest_url, ddl_info[:sqlfile]], options.merge(_init_task: false))
    cmds += invoke(:restore_data, [dest_url, data_info[:data_dir]], options.merge(_init_task: false))

    handle_cmds(cmds)
  end

  desc 'gen_datafile src_url [file]', 'generate init datafile'
  option :limit, banner: 'try num'
  option :bigger_than, banner: 'bigger than xx M'
  option :show, type: :boolean, banner: 'display current'
  def gen_datafile(src_url, file = nil)
    src_url = get_dburl(src_url)
    src_conn = Dba::ConnectionBuilder.new(url: src_url)

    if options[:show]
      dfile = dump_datafile(src_conn)
      puts "current datafile: #{dfile}"
      return
    end

    require "active_support"
    require "active_support/core_ext/numeric"

    limit = (options[:limit] || 100).to_i
    bigger_than = (options[:bigger_than] || 30).to_i * 1024 ** 2 #M

    mapping = {}
    db = src_conn.sdb
    db.user_schemata.each do |sch|
      sql = <<-SQL
        select table_schema || '.' || table_name as tbl, pg_total_relation_size(quote_ident(table_name)) 
          from information_schema.tables where table_schema in ? 
          order by 2 desc limit #{limit};
      SQL
      sql = db.fetch(sql, [sch.to_s]).sql

      tbl_mapping = {}
      db.fetch(sql) do |row|
        tbl, size = row.values
        size = size.to_i
        if size >= bigger_than
          tbl_mapping["#{tbl}"] = size.to_s(:human_size)
        end
      end
      mapping[sch] = tbl_mapping
    end

    result = {
      generated_time: Time.now,
      head_rows_tables: [],
      no_rows_tables: mapping
    }
    dfile = file || ".datafiles/#{src_conn.unique_id}.yml"
    system "mkdir -p #{File.dirname(dfile)}"
    File.write(dfile, result.to_yaml)
    puts result.to_yaml
    puts "result write to #{dfile}"
  end

  desc 'copy_table_data src_url dest_url table [dest_table]', 'copy table data between dbs'
  option :truncate, type: :boolean, default: true
  option :cascade, type: :boolean, default: true
  option :restart, type: :boolean, default: true
  option :only, type: :boolean, default: true
  option :sql_copy, type: :boolean, default: true
  option :limit, banner: 'limit count'
  option :limit_big_count, type: :boolean, default: false
  def copy_table_data(src_url, dest_url, table, dtable = table)
    unless dryrun
      unless prompter.yes?("Are you sure to truncate then append #{dtable}?")
        return 
      end
    end
    src_url = get_dburl(src_url)
    src_conn = Dba::ConnectionBuilder.new(url: src_url)
    dest_url = allow_local_db_url(dest_url)
    dest_conn = Dba::ConnectionBuilder.new(url: dest_url)
    
    limit = options[:limit]
    unless limit
      if options[:limit_big_count]
        count = src_conn.table_count(table)
        max = 2000
        if count > max
          warn "==limit 1000 for #{count} records!"
          limit = max
        end
      end
    end

    truncate = options[:truncate]
    sql_copy = options[:sql_copy]

    if sql_copy
      cmds = []
      clause = table
      clause = "(select * from #{table} limit #{limit})" if limit
      if truncate
        trunc_clause = "truncate table only #{dtable} restart identity cascade; " 
      end
      # todo 大表是否要先转存到文件以防内存徒增？？？
      cmds << "#{sql_cmd} --command 'copy #{clause} TO STDOUT;' #{src_url} | 
               #{sql_cmd} --command '#{trunc_clause} copy #{dtable} from STDIN;' #{dest_url}"
      handle_cmds(cmds)
    else
      src_dset = src_conn.sequel_db.from(table)
      src_dset = src_dset.limit(limit) if limit
      dest_dset = dest_conn.sequel_db.from(dtable)
      if truncate
        dest_dset.truncate(options.symbolize_keys.slice(:cascade, :restart, :only))
      end
      src_dset.map do |row|
        puts "==copy row: #{row.values.first}"
        unless truncate
          row = row.delete_if{|k| [:id].include?(k) }
        end
        dest_dset.insert(row)
      end
      puts "dest current item count: #{dest_dset.count}"
    end
  end

  desc 'copy_table_data2 src_url dest_url table', 'copy table data use dump method'
  def copy_table_data2(src_url, dest_url, table)
    invoke(:copy_data, [src_url, dest_url], options.merge(_init_task: false, 
                                                          dump_opts: "--table #{table}"))
  end

  desc 'copy_table src_url dest_url table', 'copy table between dbs'
  option :schema_only, type: :boolean, default: false
  option :data_only, type: :boolean, default: false
  def copy_table(src_url, dest_url, table)
    cmds = []
    src_url = allow_local_db_url(src_url)
    src_conn = Dba::ConnectionBuilder.new(url: src_url)

    dest_url = allow_local_db_url(dest_url)

    opts =  {
      _init_task: false,
      ensure_no_dest_db: false,
      dump_opts: "#{dump_opts} --table #{table}"
    }
    opts[:dump_opts] += ' --schema-only' if options[:schema_only]
    opts[:dump_opts] += ' --data-only' if options[:data_only]

    cmds += invoke(:fork, [src_url, dest_url], options.merge(opts))
    handle_cmds(cmds)
  end

  # sample: pga fork src1 dest1 --dump-opts='--schema public'
  desc 'fork src_url dest_url', 'fork db using pg_dump then restore in parallel'
  option :ensure_no_dest_db, type: :boolean, default: true
  def fork(src_url, dest_url)
    cmds = []
    src_url = allow_local_db_url(src_url)
    src_conn = Dba::ConnectionBuilder.new(url: src_url)

    dest_url = allow_local_db_url(dest_url)

    if options[:ensure_no_dest_db]
      ensure_no_dest_db(dest_url)
      unless dryrun
        dest_conn = Dba::ConnectionBuilder.new(url: dest_url)
        create_db_before_restore(dest_conn) 
      end
    end

    dump_dir = tmp_path("#{src_conn.unique_id}-fork")
    cmds << "mkdir -p #{dump_dir}"
    cmds << "#{dump_cmd} --no-owner --no-privileges #{jobs_opts} --format=d --file=#{dump_dir} #{verbose_cmd_opts} #{options[:dump_opts]} #{src_conn.url};"
    cmds << "#{restore_cmd} --no-owner --format=d #{jobs_opts} #{verbose_cmd_opts} --dbname=#{dest_url} #{dump_dir};"
    handle_cmds(cmds)
  end

  desc 'test URL', 'test url'
  def test(url)
    puts allow_local_db_url(url)
  end

  no_commands do
    def prompter
      @prompter ||= TTY::Prompt.new
    end

    def handle_dump_args(src_url, dump_dir = nil)
      if src_url.is_a?(Dba::ConnectionBuilder)
        src_conn = src_url
      else
        src_url = get_dburl(src_url)
        src_conn = Dba::ConnectionBuilder.new(url: src_url)
      end
      dump_dir ||= tmp_path(src_conn.unique_id).to_s
      cmd = "mkdir -p #{dump_dir}"
      [src_conn, dump_dir, cmd]
    end

    def handle_cmds(cmds, dry: false)
      cmds = Array(cmds) unless cmds.is_a?(Array)
      return cmds unless options[:_init_task]

      time0 = Time.now
      unless dryrun
        unless dry
          cmds.each do |cmd|
            system cmd
          end
        end
      end
      taken = Time.now - time0

      puts "# run below cmds in #{taken} seconds:"
      puts cmds.join(";\n")

      cmds
    end

    def create_db_before_restore(dest_conn)
      if dest_conn.db_not_exist?
        puts "creating new dest db: #{dest_conn.dbname}"
        dest_conn.fork(dbname: 'postgres') do |c|
          c.run_sql("create database #{dest_conn.dbname};")
        end
      else
        abort "#{dest_conn.dbname} has existed!"
      end
    end

    def dryrun; options[:dryrun] end

    def verbose_cmd_opts
      options[:verbose_cmd] ? "--verbose" : ""
    end

    def dump_opts
      options[:dump_opts]
    end

    # https://gist.github.com/jimweirich/5813834
    def jobs_opts
      nproc = ENV['JOB_NUM'] || Etc.nprocessors
      "--jobs=#{nproc}"
    end

    def exclude_opts(file)
      return '' unless file && File.exist?(file)
      yml = YAML.load_file(file)
      head_rows_tables = yml[:head_rows_tables]
      no_rows_tables = yml[:no_rows_tables].values.map(&:keys).flatten
      excluded_tables = head_rows_tables + no_rows_tables
      excluded_str = excluded_tables.map do |t|
        "--exclude-table-data=\'#{t}\'"
      end.join(' ')
      excluded_str
    end

    def dump_datafile(conn)
      return unless options[:use_datafile]
      files = []
      files << ENV['DATAFILE'] if ENV['DATAFILE']
      files.unshift(options[:datafile]) if options[:datafile]
      
      dir = '.datafiles'
      names = [conn.unique_id, conn.dbdomain, conn.dbname, "default"]
      files += names.map{ |f| File.join(dir, "#{f}.yml") }
      files.each do |f|
        return f if File.exist?(f)
      end
      nil
    end

    def check_url!(url)
      abort "Invalid url: #{url}!" unless Dba::Util.is_url?(url)
    end

    def allow_local_db_url(url)
      if Dba::Util.is_url?(url)
        url
      else
        dburl = get_dburl(url)
        return dburl if dburl
        # suppose url is dbname
        "postgres://localhost/#{url}"
      end
    end

    def ensure_no_dest_db(url)
      return if dryrun
      abort "Has existed #{url}!" if db_exist?(url)
    end

    def db_exist?(url)
      connb = url.is_a?(Dba::ConnectionBuilder) ? url : Dba::ConnectionBuilder.new(url: url)
      !connb.db_not_exist?
    end

    def get_dburl(db)
      return db if Dba::Util.is_url?(db)
      url = `dbcli url #{db}`.chomp
      url == '' ? nil : url
    end

    def tmp_path(prefix = 'pgdump')
      tstamp = Time.now.strftime("%Y%m%d%H%M%S")
      Pathname("tmp/#{prefix}-dump-#{tstamp}")
    end

    def dump_cmd
      'pg_dump'
    end

    def sql_cmd
      'psql --no-psqlrc'
    end

    def restore_cmd
      'pg_restore'
    end
  end
end

PgaCLI.start
